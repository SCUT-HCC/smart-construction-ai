"""qmd ç«¯åˆ°ç«¯é›†æˆéªŒè¯è„šæœ¬ã€‚

éªŒè¯é€‰å®šçš„ Qwen3-Embedding-0.6B + Qwen3-Reranker-0.6B æ¨¡å‹ç»„åˆ
ä¸ qmd + sqlite-vec çš„å®Œæ•´é›†æˆå…¼å®¹æ€§ã€‚

éªŒè¯é¡¹ï¼š
  1. SentenceTransformerBackend åŠ è½½ Qwen3-Embedding-0.6B
  2. embed() / embed_batch() è¿”å›æ­£ç¡®ç»´åº¦ï¼ˆ1024ï¼‰å‘é‡
  3. sqlite-vec å­˜å‚¨å’Œæ£€ç´¢ 1024 ç»´å‘é‡
  4. ç«¯åˆ°ç«¯æµç¨‹ï¼šindex â†’ embed â†’ search â†’ è¿”å›ç»“æœ
  5. Qwen3-Reranker-0.6B CausalLM åŠ è½½ + æ‰“åˆ†éªŒè¯
  6. åŒæ¨¡å‹åŒæ—¶åŠ è½½æ˜¾å­˜éªŒè¯

ç”¨æ³•ï¼š
    conda run -n sca python scripts/verify_qmd_integration.py
    conda run -n sca python scripts/verify_qmd_integration.py --fragments docs/knowledge_base/fragments/fragments.jsonl
"""

import argparse
import json
import sys
import tempfile
from pathlib import Path
from typing import Any

import torch

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ---------------------------------------------------------------------------
# é»˜è®¤é…ç½®ï¼ˆK20 è¯„æµ‹é€‰å®šç»„åˆï¼‰
# ---------------------------------------------------------------------------

DEFAULT_EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"
DEFAULT_RERANKER_MODEL = "Qwen/Qwen3-Reranker-0.6B"
EXPECTED_DIM = 1024

# æ–½å·¥æ–¹æ¡ˆé¢†åŸŸæµ‹è¯•æ•°æ®
BUILTIN_DOCS = [
    {
        "collection": "ch06_methods",
        "id": "test_ch6_s01",
        "content": (
            "æ··å‡åœŸæµ‡ç­‘æ–½å·¥å·¥è‰ºï¼šé‡‡ç”¨å•†å“æ··å‡åœŸï¼Œå¼ºåº¦ç­‰çº§C30ï¼Œåè½åº¦æ§åˆ¶åœ¨160Â±20mmã€‚"
            "æµ‡ç­‘å‰åº”å¯¹æ¨¡æ¿ã€é’¢ç­‹è¿›è¡Œéšè”½å·¥ç¨‹éªŒæ”¶ã€‚æµ‡ç­‘æ—¶åº”åˆ†å±‚æµ‡ç­‘ï¼Œæ¯å±‚åšåº¦ä¸è¶…è¿‡500mmï¼Œ"
            "æŒ¯æ£æ£’æ’å…¥ä¸‹å±‚50mmï¼ŒæŒ¯æ£è‡³æ··å‡åœŸè¡¨é¢ä¸å†ä¸‹æ²‰ã€æ— æ°”æ³¡å†’å‡ºä¸ºæ­¢ã€‚"
        ),
    },
    {
        "collection": "ch06_methods",
        "id": "test_ch6_s02",
        "content": (
            "é’¢ç»“æ„åŠè£…æ–½å·¥æ–¹æ¡ˆï¼šé‡‡ç”¨QY160æ±½è½¦èµ·é‡æœºè¿›è¡Œæ„ä»¶åŠè£…ï¼ŒåŠè£…å‰åº”æ£€æŸ¥åŠå…·ã€ç´¢å…·çš„"
            "å®Œå¥½æ€§ï¼Œç¡®è®¤èµ·é‡æœºçš„ç¨³å®šæ€§ã€‚æ„ä»¶å°±ä½åç«‹å³è¿›è¡Œä¸´æ—¶å›ºå®šï¼Œé«˜å¼ºèºæ “åˆæ‹§æ‰­çŸ©åº”ç¬¦åˆ"
            "è®¾è®¡è¦æ±‚ã€‚æ‰€æœ‰åŠè£…ä½œä¸šåº”åœ¨é£é€Ÿä¸è¶…è¿‡å…­çº§æ—¶è¿›è¡Œã€‚"
        ),
    },
    {
        "collection": "ch07_quality",
        "id": "test_ch7_s01",
        "content": (
            "æ··å‡åœŸè´¨é‡æ§åˆ¶æªæ–½ï¼šæ¯100mÂ³å–æ ·ä¸€ç»„æ ‡å‡†å…»æŠ¤è¯•ä»¶ï¼ŒåŒæ¡ä»¶å…»æŠ¤è¯•ä»¶ä¸ç»“æ„åŒæ¡ä»¶æ”¾ç½®ã€‚"
            "åè½åº¦æ¯è½¦æ£€æµ‹ï¼Œåå·®è¶…è¿‡Â±30mmçš„é€€å›ã€‚æŒ¯æ£è´¨é‡ç”±ä¸“äººè´Ÿè´£æ—ç«™ç›‘ç£ã€‚"
        ),
    },
    {
        "collection": "ch08_safety",
        "id": "test_ch8_s01",
        "content": (
            "é«˜å¤„ä½œä¸šå®‰å…¨æªæ–½ï¼šä½œä¸šäººå‘˜å¿…é¡»ä½©æˆ´å®‰å…¨å¸¦ï¼Œå®‰å…¨å¸¦åº”é«˜æŒ‚ä½ç”¨ã€‚ä¸´è¾¹é˜²æŠ¤æ æ†é«˜åº¦ä¸ä½äº"
            "1.2ç±³ï¼Œè®¾ç½®ä¸Šä¸‹ä¸¤é“æ¨ªæ†å’ŒæŒ¡è„šæ¿ã€‚å¤œé—´æ–½å·¥åº”è®¾ç½®è¶³å¤Ÿç…§æ˜ï¼Œç…§åº¦ä¸ä½äº50lxã€‚"
        ),
    },
]

BUILTIN_QUERIES = [
    ("æ··å‡åœŸæµ‡ç­‘çš„æŒ¯æ£è¦æ±‚æ˜¯ä»€ä¹ˆ", "test_ch6_s01"),
    ("é«˜å¤„ä½œä¸šéœ€è¦å“ªäº›å®‰å…¨é˜²æŠ¤æªæ–½", "test_ch8_s01"),
    ("é’¢ç»“æ„åŠè£…ä½¿ç”¨ä»€ä¹ˆèµ·é‡è®¾å¤‡", "test_ch6_s02"),
]


# ---------------------------------------------------------------------------
# å·¥å…·å‡½æ•°
# ---------------------------------------------------------------------------


def _get_vram_mb() -> float:
    """è·å–å½“å‰ GPU æ˜¾å­˜å ç”¨ï¼ˆMBï¼‰ã€‚"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024 / 1024
    return 0.0


def _print_result(name: str, passed: bool, detail: str = "") -> None:
    """æ‰“å°éªŒè¯ç»“æœã€‚"""
    status = "âœ… PASS" if passed else "âŒ FAIL"
    msg = f"  {status}  {name}"
    if detail:
        msg += f"  ({detail})"
    print(msg)


def load_jsonl(path: str) -> list[dict[str, Any]]:
    """åŠ è½½ JSONL æ–‡ä»¶ã€‚"""
    items: list[dict[str, Any]] = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                items.append(json.loads(line))
    return items


# ---------------------------------------------------------------------------
# éªŒè¯æ­¥éª¤
# ---------------------------------------------------------------------------


def verify_embedding_load(model_name: str) -> tuple[bool, int]:
    """éªŒè¯ 1: SentenceTransformerBackend åŠ è½½åµŒå…¥æ¨¡å‹ã€‚

    Args:
        model_name: HuggingFace æ¨¡å‹åç§°

    Returns:
        (é€šè¿‡/å¤±è´¥, å‘é‡ç»´åº¦)
    """
    from qmd.llm.sentence_tf import SentenceTransformerBackend

    backend = SentenceTransformerBackend(model_name=model_name, device="cuda")
    dim = backend.get_embedding_dimensions()
    passed = dim == EXPECTED_DIM
    _print_result("Embedding æ¨¡å‹åŠ è½½", passed, f"ç»´åº¦={dim}, é¢„æœŸ={EXPECTED_DIM}")
    return passed, dim


def verify_embed_ops(model_name: str) -> bool:
    """éªŒè¯ 2: embed() å’Œ embed_batch() æ­£ç¡®æ€§ã€‚

    Args:
        model_name: HuggingFace æ¨¡å‹åç§°

    Returns:
        é€šè¿‡/å¤±è´¥
    """
    from qmd.llm.sentence_tf import SentenceTransformerBackend

    backend = SentenceTransformerBackend(model_name=model_name, device="cuda")

    # å•æ¡åµŒå…¥
    result_single = backend.embed("æ··å‡åœŸæµ‡ç­‘æ–½å·¥å·¥è‰ºè¦æ±‚", is_query=True)
    if result_single is None:
        _print_result("embed() å•æ¡", False, "è¿”å› None")
        return False

    dim = len(result_single.embedding)
    single_ok = dim == EXPECTED_DIM and result_single.model == model_name
    _print_result("embed() å•æ¡", single_ok, f"ç»´åº¦={dim}, model={result_single.model}")

    # æ‰¹é‡åµŒå…¥
    texts = [d["content"] for d in BUILTIN_DOCS]
    results_batch = backend.embed_batch(texts)
    batch_ok = (
        len(results_batch) == len(texts)
        and all(r is not None for r in results_batch)
        and all(len(r.embedding) == EXPECTED_DIM for r in results_batch if r is not None)
    )
    _print_result("embed_batch() æ‰¹é‡", batch_ok, f"{len(results_batch)} æ¡, ç»´åº¦å‡ä¸º {EXPECTED_DIM}")

    return single_ok and batch_ok


def verify_sqlite_vec(model_name: str) -> bool:
    """éªŒè¯ 3: sqlite-vec å­˜å‚¨ + æ£€ç´¢ + qmd search ç«¯åˆ°ç«¯ã€‚

    Args:
        model_name: åµŒå…¥æ¨¡å‹åç§°

    Returns:
        é€šè¿‡/å¤±è´¥
    """
    import qmd
    from qmd.llm.sentence_tf import SentenceTransformerBackend

    with tempfile.TemporaryDirectory() as tmp_dir:
        db_path = Path(tmp_dir) / "test.db"
        db, store = qmd.create_store(str(db_path))

        # ç´¢å¼•æ–‡æ¡£
        for doc in BUILTIN_DOCS:
            store.index_document(doc["collection"], doc["id"], doc["content"])
        doc_count = store.get_document_count("ch06_methods")

        # åµŒå…¥
        backend = SentenceTransformerBackend(model_name=model_name, device="cuda")
        store.embed_documents(backend, force=False, batch_size=2)

        # æ£€ç´¢ï¼ˆéœ€ä¼ å…¥ llm_backend ä»¥å¯ç”¨å‘é‡æ£€ç´¢ï¼‰
        results = qmd.search(
            db, "æ··å‡åœŸæµ‡ç­‘æŒ¯æ£è¦æ±‚", collection="ch06_methods", limit=3, llm_backend=backend
        )

        passed = doc_count > 0 and len(results) > 0
        top_body = results[0].body[:50] if results and results[0].body else "N/A"
        _print_result(
            "sqlite-vec å­˜å‚¨+æ£€ç´¢",
            passed,
            f"ç´¢å¼• {doc_count} æ¡, æ£€ç´¢ {len(results)} æ¡, top1={top_body}...",
        )
        return passed


def verify_e2e_accuracy(model_name: str) -> bool:
    """éªŒè¯ 4: ç«¯åˆ°ç«¯æ£€ç´¢å‡†ç¡®æ€§ï¼ˆå†…ç½®æµ‹è¯•ç”¨ä¾‹ï¼‰ã€‚

    Args:
        model_name: åµŒå…¥æ¨¡å‹åç§°

    Returns:
        é€šè¿‡/å¤±è´¥
    """
    import qmd
    from qmd.llm.sentence_tf import SentenceTransformerBackend

    with tempfile.TemporaryDirectory() as tmp_dir:
        db_path = Path(tmp_dir) / "test_e2e.db"
        db, store = qmd.create_store(str(db_path))

        for doc in BUILTIN_DOCS:
            store.index_document(doc["collection"], doc["id"], doc["content"])

        backend = SentenceTransformerBackend(model_name=model_name, device="cuda")
        store.embed_documents(backend, force=False, batch_size=2)

        correct = 0
        for query, expected_id in BUILTIN_QUERIES:
            results = qmd.search(db, query, limit=3, llm_backend=backend)
            top_bodies = [r.body for r in results[:3] if r.body]
            # æŸ¥æ‰¾é¢„æœŸæ–‡æ¡£å†…å®¹çš„å…³é”®å‰ç¼€
            expected_content = next(d["content"][:30] for d in BUILTIN_DOCS if d["id"] == expected_id)
            found = any(expected_content[:20] in b for b in top_bodies)
            if found:
                correct += 1

        accuracy = correct / len(BUILTIN_QUERIES)
        passed = accuracy >= 0.66
        _print_result(
            "ç«¯åˆ°ç«¯æ£€ç´¢å‡†ç¡®æ€§",
            passed,
            f"{correct}/{len(BUILTIN_QUERIES)} æ­£ç¡® ({accuracy:.0%})",
        )
        return passed


def verify_reranker_causal(model_name: str) -> bool:
    """éªŒè¯ 5: Qwen3-Reranker CausalLM æ¨¡å‹åŠ è½½ + yes/no æ‰“åˆ†ã€‚

    Qwen3-Reranker ä½¿ç”¨ CausalLM æ¶æ„ï¼ˆé CrossEncoderï¼‰ï¼Œ
    é€šè¿‡ yes/no token çš„ log_softmax æ¦‚ç‡ä½œä¸ºç›¸å…³æ€§åˆ†æ•°ã€‚

    Args:
        model_name: Reranker æ¨¡å‹åç§°

    Returns:
        é€šè¿‡/å¤±è´¥
    """
    from transformers import AutoModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")
    model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16
    ).cuda().eval()

    # éªŒè¯ yes/no token å­˜åœ¨
    yes_id = tokenizer.convert_tokens_to_ids("yes")
    no_id = tokenizer.convert_tokens_to_ids("no")
    vocab_ok = yes_id != tokenizer.unk_token_id and no_id != tokenizer.unk_token_id
    _print_result("Reranker yes/no token", vocab_ok, f"yes={yes_id}, no={no_id}")

    if not vocab_ok:
        del model
        torch.cuda.empty_cache()
        return False

    # æµ‹è¯•æ‰“åˆ†
    instruction = "ç»™å®šä¸€ä¸ªæ–½å·¥æ–¹æ¡ˆç›¸å…³çš„æ£€ç´¢æŸ¥è¯¢ï¼Œåˆ¤æ–­æ–‡æ¡£æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³"
    query = "æ··å‡åœŸæµ‡ç­‘çš„æŒ¯æ£è¦æ±‚"
    doc_pos = BUILTIN_DOCS[0]["content"]  # æ··å‡åœŸæµ‡ç­‘ â†’ ç›¸å…³
    doc_neg = BUILTIN_DOCS[3]["content"]  # é«˜å¤„ä½œä¸šå®‰å…¨ â†’ ä¸ç›¸å…³

    prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
    suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"

    scores: list[float] = []
    for doc in [doc_pos, doc_neg]:
        content = f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
        text = prefix + content + suffix
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=4096).to("cuda")
        with torch.no_grad():
            logits = model(**inputs).logits[:, -1, :]
        true_score = logits[:, yes_id]
        false_score = logits[:, no_id]
        stacked = torch.stack([false_score, true_score], dim=1)
        probs = torch.nn.functional.log_softmax(stacked, dim=1)
        score = probs[:, 1].exp().item()
        scores.append(score)

    score_ok = scores[0] > scores[1]  # ç›¸å…³æ–‡æ¡£åˆ†æ•°åº”é«˜äºä¸ç›¸å…³
    _print_result(
        "Reranker CausalLM æ‰“åˆ†",
        score_ok,
        f"ç›¸å…³={scores[0]:.4f} > ä¸ç›¸å…³={scores[1]:.4f}",
    )

    del model
    torch.cuda.empty_cache()

    print("  âš ï¸  æ³¨æ„: qmd å½“å‰ rerank() ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œéœ€å®ç° Qwen3CausalLMBackend ä»¥é›†æˆ")
    return vocab_ok and score_ok


def verify_vram_budget(embedding_model: str, reranker_model: str) -> bool:
    """éªŒè¯ 6: åŒæ¨¡å‹åŒæ—¶åŠ è½½çš„æ˜¾å­˜é¢„ç®—ï¼ˆéœ€ < 24GBï¼‰ã€‚

    Args:
        embedding_model: åµŒå…¥æ¨¡å‹åç§°
        reranker_model: Reranker æ¨¡å‹åç§°

    Returns:
        é€šè¿‡/å¤±è´¥
    """
    from qmd.llm.sentence_tf import SentenceTransformerBackend
    from transformers import AutoModelForCausalLM, AutoTokenizer

    vram_before = _get_vram_mb()

    # åŠ è½½ Embedding
    backend = SentenceTransformerBackend(model_name=embedding_model, device="cuda")
    _ = backend.embed("test", is_query=True)
    vram_after_emb = _get_vram_mb()
    emb_vram = vram_after_emb - vram_before

    # åŠ è½½ Reranker
    tokenizer = AutoTokenizer.from_pretrained(reranker_model, padding_side="left")
    model = AutoModelForCausalLM.from_pretrained(
        reranker_model, torch_dtype=torch.float16
    ).cuda().eval()
    vram_after_both = _get_vram_mb()
    rr_vram = vram_after_both - vram_after_emb
    total = vram_after_both - vram_before

    del model, tokenizer
    torch.cuda.empty_cache()

    passed = total < 24000
    _print_result(
        "æ˜¾å­˜é¢„ç®— (<24GB)",
        passed,
        f"Embedding={emb_vram:.0f}MB + Reranker={rr_vram:.0f}MB = {total:.0f}MB",
    )
    return passed


# ---------------------------------------------------------------------------
# ä¸»æµç¨‹
# ---------------------------------------------------------------------------


def main() -> None:
    """è¿è¡Œæ‰€æœ‰éªŒè¯é¡¹ã€‚"""
    parser = argparse.ArgumentParser(description="K20 qmd é›†æˆéªŒè¯")
    parser.add_argument(
        "--embedding-model",
        default=DEFAULT_EMBEDDING_MODEL,
        help=f"åµŒå…¥æ¨¡å‹ (é»˜è®¤: {DEFAULT_EMBEDDING_MODEL})",
    )
    parser.add_argument(
        "--reranker-model",
        default=DEFAULT_RERANKER_MODEL,
        help=f"Reranker æ¨¡å‹ (é»˜è®¤: {DEFAULT_RERANKER_MODEL})",
    )
    parser.add_argument(
        "--fragments",
        default=None,
        help="å¯é€‰: å¤–éƒ¨ JSONL ç‰‡æ®µæ–‡ä»¶è·¯å¾„ï¼ˆä¸æä¾›åˆ™ç”¨å†…ç½®æµ‹è¯•æ•°æ®ï¼‰",
    )
    args = parser.parse_args()

    print("=" * 60)
    print("K20 qmd é›†æˆéªŒè¯")
    print(f"  åµŒå…¥æ¨¡å‹: {args.embedding_model}")
    print(f"  Reranker: {args.reranker_model}")
    print(f"  é¢„æœŸç»´åº¦: {EXPECTED_DIM}")
    print("=" * 60)

    steps: list[tuple[str, Any]] = [
        ("1. Embedding æ¨¡å‹åŠ è½½", lambda: verify_embedding_load(args.embedding_model)[0]),
        ("2. embed()/embed_batch()", lambda: verify_embed_ops(args.embedding_model)),
        ("3. sqlite-vec å­˜å‚¨+æ£€ç´¢", lambda: verify_sqlite_vec(args.embedding_model)),
        ("4. ç«¯åˆ°ç«¯æ£€ç´¢å‡†ç¡®æ€§", lambda: verify_e2e_accuracy(args.embedding_model)),
        ("5. Reranker CausalLM", lambda: verify_reranker_causal(args.reranker_model)),
        (
            "6. æ˜¾å­˜é¢„ç®—",
            lambda: verify_vram_budget(args.embedding_model, args.reranker_model),
        ),
    ]

    results: list[tuple[str, bool]] = []
    for name, func in steps:
        print(f"\n[{name}]")
        try:
            passed = func()
            results.append((name, passed))
        except Exception as e:
            print(f"  âŒ FAIL  å¼‚å¸¸: {e}")
            results.append((name, False))

    # æ±‡æ€»
    print("\n" + "=" * 60)
    print("éªŒè¯æ±‡æ€»")
    print("=" * 60)
    passed_count = sum(1 for _, p in results if p)
    for name, passed in results:
        status = "âœ…" if passed else "âŒ"
        print(f"  {status} {name}")
    print(f"\n  ç»“æœ: {passed_count}/{len(results)} é€šè¿‡")

    if passed_count == len(results):
        print("\n  ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼æ¨¡å‹ç»„åˆå¯é›†æˆåˆ° qmdã€‚")
    else:
        print("\n  âš ï¸  éƒ¨åˆ†éªŒè¯æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥å¤±è´¥é¡¹ã€‚")
        sys.exit(1)


if __name__ == "__main__":
    main()
